{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'catr' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/saahiluppal/catr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from -r requirements.txt (line 1)) (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from -r requirements.txt (line 2)) (0.13.1+cu116)\n",
      "Requirement already satisfied: numpy in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from -r requirements.txt (line 3)) (1.23.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from -r requirements.txt (line 4)) (4.21.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from -r requirements.txt (line 5)) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from torchvision->-r requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from transformers->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from transformers->-r requirements.txt (line 4)) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from transformers->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from transformers->-r requirements.txt (line 4)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from transformers->-r requirements.txt (line 4)) (2022.7.25)\n",
      "Requirement already satisfied: filelock in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from transformers->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from tqdm->-r requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\takah\\\\Desktop\\\\python_files\\\\bokete'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"./catr\")\n",
    "os.getcwd()\n",
    "!pip install -r requirements.txt\n",
    "os.chdir(\"../\")\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from catr.models import caption\n",
    "from catr.datasets import coco, utils\n",
    "from catr.configuration import Config\n",
    "import os\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Image Captioning')\n",
    "# parser.add_argument('--path', type=str, help='path to image', required=True)\n",
    "# parser.add_argument('--v', type=str, help='version', default='v3')\n",
    "# parser.add_argument('--checkpoint', type=str, help='checkpoint path', default=None)\n",
    "# args = parser.parse_args()\n",
    "# image_path = args.path\n",
    "# version = args.v\n",
    "# checkpoint_path = args.checkpoint\n",
    "class catr_model:\n",
    "    def __init__(self):\n",
    "        version = \"v3\"\n",
    "        checkpoint_path = None\n",
    "\n",
    "        self.config = Config()\n",
    "\n",
    "        if version == 'v1':\n",
    "            self.model = torch.hub.load('saahiluppal/catr', 'v1', pretrained=True)\n",
    "        elif version == 'v2':\n",
    "            self.model = torch.hub.load('saahiluppal/catr', 'v2', pretrained=True)\n",
    "        elif version == 'v3':\n",
    "            self.model = torch.hub.load('saahiluppal/catr', 'v3', pretrained=True)\n",
    "        else:\n",
    "            print(\"Checking for checkpoint.\")\n",
    "            if checkpoint_path is None:\n",
    "                raise NotImplementedError('No model to chose from!')\n",
    "            else:\n",
    "                if not os.path.exists(checkpoint_path):\n",
    "                    raise NotImplementedError('Give valid checkpoint path')\n",
    "                print(\"Found checkpoint! Loading!\")\n",
    "                model,_ = caption.build_model(self.config)\n",
    "                print(\"Loading Checkpoint...\")\n",
    "                checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "                model.load_state_dict(checkpoint['model'])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.start_token = self.tokenizer.convert_tokens_to_ids(self.tokenizer._cls_token)\n",
    "        self.end_token = self.tokenizer.convert_tokens_to_ids(self.tokenizer._sep_token)\n",
    "\n",
    "    def fit(self,image_path):\n",
    "        self.image = Image.open(image_path)\n",
    "        self.image = coco.val_transform(self.image)\n",
    "        self.image = self.image.unsqueeze(0)\n",
    "\n",
    "        self.caption, self.cap_mask = self.create_caption_and_mask(\n",
    "        self.start_token, self.config.max_position_embeddings)\n",
    "\n",
    "        output = self.evaluate()\n",
    "        result = self.tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\n",
    "        #result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return (result.capitalize())\n",
    "\n",
    "    def create_caption_and_mask(self,start_token, max_length):\n",
    "        caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "        mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "        caption_template[:, 0] = start_token\n",
    "        mask_template[:, 0] = False\n",
    "\n",
    "        return caption_template, mask_template\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        for i in range(self.config.max_position_embeddings - 1):\n",
    "            predictions = self.model(self.image, self.caption, self.cap_mask)\n",
    "            predictions = predictions[:, i, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1)\n",
    "\n",
    "            if predicted_id[0] == 102:\n",
    "                return self.caption\n",
    "\n",
    "            self.caption[:, i+1] = predicted_id[0]\n",
    "            self.cap_mask[:, i+1] = False\n",
    "\n",
    "        return self.caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\takah/.cache\\torch\\hub\\saahiluppal_catr_master\n",
      "c:\\Users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\takah\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "imcap = catr_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takah/.cache\\torch\\hub\\saahiluppal_catr_master\\models\\position_encoding.py:38: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A girl is holding a colorful umbrella in her hand.\n"
     ]
    }
   ],
   "source": [
    "imcap.fit(\"../data/bokete/train/00fx15srqs.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "caption_template[:, 0] = start_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/bokete/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/bokete/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipファイルのパス\n",
    "zip_path = '../data/bokete/train.zip'\n",
    "\n",
    "# 配列格納用のList\n",
    "train_imgs = []\n",
    "\n",
    "# zipの読み込み\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "    # zipファイル内の各ファイルについてループ\n",
    "    for info in zip_file.infolist():\n",
    "        # 「zipファイル名/」については処理をしない\n",
    "        if (info.filename != 'data/'):\n",
    "            # 対象の画像ファイルを開く\n",
    "            with zip_file.open(info.filename) as img_file:\n",
    "                # 画像のバイナリデータを読み込む\n",
    "                img_bin = io.BytesIO(img_file.read())\n",
    "                # バイナリデータをpillowから開く\n",
    "                img = Image.open(img_bin)\n",
    "                # 画像データを配列化\n",
    "                img_array = np.array(img)\n",
    "                # 格納用のListに追加\n",
    "                train_imgs.append(img_array)\n",
    "\n",
    "# 処理が完了後、np.arrayに変換\n",
    "train_imgs = np.array(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipファイルのパス\n",
    "zip_path = '../data/bokete/test.zip'\n",
    "\n",
    "# 配列格納用のList\n",
    "test_imgs = []\n",
    "\n",
    "# zipの読み込み\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "    # zipファイル内の各ファイルについてループ\n",
    "    for info in zip_file.infolist():\n",
    "        # 「zipファイル名/」については処理をしない\n",
    "        if (info.filename != 'data/'):\n",
    "            # 対象の画像ファイルを開く\n",
    "            with zip_file.open(info.filename) as img_file:\n",
    "                # 画像のバイナリデータを読み込む\n",
    "                img_bin = io.BytesIO(img_file.read())\n",
    "                # バイナリデータをpillowから開く\n",
    "                img = Image.open(img_bin)\n",
    "                # 画像データを配列化\n",
    "                img_array = np.array(img)\n",
    "                # 格納用のListに追加\n",
    "                test_imgs.append(img_array)\n",
    "\n",
    "# 処理が完了後、np.arrayに変換\n",
    "test_imgs = np.array(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from tasks.mm_tasks.refcoco import RefcocoTask\n",
    " \n",
    "from models.ofa import OFAModel\n",
    "from PIL import Image\n",
    " \n",
    "import cv2\n",
    "import numpy\n",
    " \n",
    "tasks.register_task('refcoco', RefcocoTask)\n",
    " \n",
    "# turn on cuda if GPU is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use fp16 only when GPU is available\n",
    "use_fp16 = False\n",
    " \n",
    "# specify some options for evaluation\n",
    "parser = options.get_generation_parser()\n",
    "input_args = [\"\", \"--task=refcoco\", \"--beam=10\", \"--path=checkpoints/ofa_large.pt\", \"--bpe-dir=utils/BPE\"]\n",
    "args = options.parse_args_and_arch(parser, input_args)\n",
    "cfg = convert_namespace_to_omegaconf(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "139e08609a3fbce2fbc54fac7bce5ae0399bcfe4096df2489efaad135911dbcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
